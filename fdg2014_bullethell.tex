\documentclass{sig-alternate}

\usepackage{todonotes}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\newcommand{\mytodo}[1]{\textbf{[[#1]]}}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{Foundations of Digital Games}{???}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Automatic Playtesting for Game Parameter Tuning via Active Learning}


\numberofauthors{1} 

\author{
\alignauthor
anonymous
%Alexander Zook and Mark O. Riedl\\
%       \affaddr{School of Interactive Computing, College of Computing}\\
%       \affaddr{Georgia Institute of Technology}\\
%       \affaddr{Atlanta, Georgia, USA}\\
%       \email{\{a.zook, riedl\}@gatech.edu}
}


\maketitle
\begin{abstract}
Game designers use human playtesting to gather feedback about game design elements when iteratively improving a game.
Playtesting, however, is expensive: human testers must be recruited, playtest results must be aggregated and interpreted, and changes to game designs must be extrapolated from these results.
Can automated methods reduce this expense?
We show how active learning techniques can formalize and automate a subset of playtesting goals.
Specifically, we focus on the low-level parameter tuning required to balance a game once the mechanics have been chosen.
Through a case study on a shoot-`em-up game we demonstrate the efficacy of active learning to reduce the amount of playtesting needed to choose the optimal set of game parameters for a given (formal) design objective.
This work opens the potential for additional methods to reduce the human burden of performing playtesting for a variety of relevant design concerns.
\end{abstract}

% A category with the (minimum) three required fields
\category{\todo[inline]{pick}H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\terms{\todo[inline]{pick}}

\keywords{\todo[inline]{pick}}

\section{Introduction}

\todo[inline]{update top part w/FDG info}


% motivation
Iterative game design practices emphasize the centrality of playtesting to improve and refine a game's design.
Human playtesters provide valuable feedback on audience reactions to a game.
Playtesting is often claimed to be ``the single most important activity a designer engages in'' \cite{fullerton2008:playcentric}.
Test data informs designers of how real players may react to the game in ways that self-testing, simulations, and design analysis may not.
Playtesting, however, is expensive---developers must recruit players, devise design experiments, collect game play and subjective feedback data, and make design changes to meet design goals.

%  other quotes:
%``You might be thinking: But testing is an expensive process, isn't it?'' p.248

% problem
We ask the question: can we reduce the cost of the playtesting process by automating some of the more mundane aspects of playtesting?
To address this problem we focus on a subset of playtesting questions focused on ``parameter tuning.''
Parameter tuning involves making low-level changes to game mechanic settings such as character movement parameters, power-up item effects, or control sensitivity.
Games based on careful timing and reflexes depend on well-tuned parameters, including racing, platforming, shoot-`em-up, and fighting game genres.
Addressing the problem of parameter tuning requires a means to automatically select a set of potentially good parameter settings, test those settings with humans, evaluate the human results, and repeat the process until a pre-defined design goal is achieved.


% contributions
Our primary insight is to model playtesting as a form of active learning (AL).
Active learning \cite{settles2012:al-book} selects among a set of possible inputs to get the best output while minimizing the number of inputs tested.
We define the ``best output'' in terms of a parameter tuning design goal and treat a setting for a game design as an ``input.''
Minimizing the number of inputs tested minimizes the number of playtests performed.
This paper makes three contributions toward machine-driven playtesting:
\begin{enumerate}
\item Machine-driven playtesting---a formulation of efficient playtesting as an AL problem % TODO: "active playtesting"? "machine-driven playtesting"? needs a better name
\item A definition of a set of playtesting goals in terms of AL metrics
\item A case study of a shoot-`em-up game demonstrating the efficacy of AL to reduce the number of playtests needed to optimize (1) difficulty-related and (2) control-related game parameters
\end{enumerate}

% differences from prior work
We believe machine-driven playtesting is a novel use of machine learning in games.
Unlike prior work in dynamic difficulty and adaptive games we focus on the case of deciding on a fixed design for future use.
Our approach can be applied to online game adjustments to more rapidly converge on the right set of game parameters.
%Further, by learning a model of how design parameters influence playtest results our approach is able to provide designers additional feedback on their ``design space'' at design time.
%A design space can be visualized to understand the tradeoffs among different parameter settings.
Unlike prior work on game design support tools using simulations or model-checking we focus on the problem of efficiently working with a set of human testers.
Our approach complements these tools for early-stage exploration with late-stage refinement.

% roadmap
In this paper we first compare our approach to related work on game design support.
We next define a set of parameter tuning design goals and relate them to AL approaches.
Following this we describe a case study of automated playtesting using a shoot-`em-up game.
After describing the game used we present results showing the efficacy of AL reduce playtesting costs using human data collected from an online study.
We conclude with a discussion of the limitations, applications, and potential extensions to this playtesting approach.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}

% overview
Two research areas are closely related to machine playtesting: offline game design tools and online game adaptation.
Offline game design tools enable designers to explore possible game designs by defining a high-level space of games through a design language.
%Examples include support for generating or evaluating design in terms of hard constraints (what must be true) or soft optimization (what to achieve to the best degree possible).
Online game adaptation changes game designs in real-time based on player actions or game state.
%Examples include designer authored rules or data-driven player models coupled to optimization techniques.



\subsection{Offline Game Design Tools}
% offline design: model-checking vs simulation
Offline game design tools have evaluated game designs using simulated players and formal model-checking.
Simulation-based tools use sampling techniques to test aspects of a game design for ``playability,'' typically defined as the ability for a player to reach a given goal state with the current design parameters.
Model-checking tools define game mechanics in a logical language to provide hard guarantees on the same kinds of playability tests.

% simulation
Bauer et al. \cite{bauer2013:rrt-generation} and Cook et al. \cite{cook2012:coopcoevo} use sampling methods to evaluate platformer game level playability.
Shaker et al. \cite{shaker2013:ropossum-test} combine a rule-based reasoning approach with simulation to generate content for a physics-based game.
Simulation approaches are valuable when design involves an intractably large space of possible parameters to test and can serve as input to optimization techniques.
%
% model-checking
Model-checking approaches provide guarantees on generated designs having formally defined properties---typically at the cost of being limited to more coarse design parameter decisions.
Smith et al. \cite{smith2013:quantify-play}, Butler et al. \cite{butler2013:progression-tool}, and Horswill and Foged \cite{horswill2012:levelgen} use logic programming and constraint solving to generating levels or sets of levels meeting given design constraints.
Jaffe et al. \cite{jaffe2012:balance} use game-theoretic analysis to understand the effects of game design parameters on competitive game balance.

% differences of playtest vs model-check -> value to designers
Our approach to automated playtesting is intended to complement these approaches to high-level early design exploration with low-level optimization of game parameters and tuning.
Further, we focus on a generic technique that applies to cases with human testers in the loop, crucial to tuning game controls or subjective features of games.
%
Offline design tools currently enable designers to formally define and enforce properties of a game design across all possible player behaviors in a specific game or space of designs.
To date these tools have emphasized techniques for ensuring game designs have desired formal properties that meet designer intent.
%
% note: not about model-checking vs playtesting but expanding offline tools available
Machine-driven playtesting enables players to provide feedback to designers on \textit{expected} player behaviors in a game.
Developing machine-driven playtesting techniques affords designers insight into how human audiences interact with designer intent, complementing an understanding of whether and how a game matches formal criteria.




\subsection{Online Game Adaptation}
%% online adaptation: hand-crafted rules vs ML/EC
% hand-crafted rules
Online game adaptation researchers have used both hand-crafted rules and data-driven techniques.
Hunicke and Chapman \cite{hunicke2004:dda} tracked the average and variance of player damage and inventory levels and employ a hand-crafted policy to adjust levels of enemies or powerups. 
Systems by Magerko et al. \cite{magerko2006:isat}, El-Nasr \cite{seifel-nasr2007:mirage}, and Thue et al. \cite{thue2007:storytell-pm} model players as vectors of skills, personality traits, or pre-defined ``player types'' and select content to fit players using hand-crafted rules. % note: erring on side of more for now
Hand-crafted rules enable designers to describe fine-tuned details of how to adjust a game toward design goals.
However, designers must fully describe how to tune the game and rules are often sensitive to minor changes in game settings.

% ML / EC
To bypass the brittleness of rules others have employed data-driven techniques that optimize game parameters toward design goals.
Hastings et al. \cite{hastings2009:gar}, Shaker et al. \cite{shaker2013:crowdsource-platform-aesthetics}, Liapis et al. \cite{liapis2013:rank-based-interactive-evol} and Yu and Riedl \cite{yu2013:storyeti} model player preferences using neuro-evolutionary or machine learning techniques and optimize the output of these models to select potential game parameters.
Harrison and Roberts \cite{harrison2013:scrabble-retention} optimize player retention and Zook and Riedl \cite{zook2012:tf} optimize game difficulty using similar techniques.

%Both rule-based and data-driven online adaptation approaches target real-time adjustments of a game to known design goals.
%Active learning can enhance simulation approaches by guiding the sampling process toward spaces of parameters likely to be of greater value.
Automated playtesting extends these approaches with principled methods to guide the process of designing hand-crafted rules or optimizing game parameters.
When hand-crafting rules, automated playtesting informs the choice of which rule parameters to use.
When optimizing models learned from data, automated playtesting informs the choice of which next set of parameters to test during the optimization process.
We argue research to date has ignored the problem of reducing ``sample complexity''---the number of data points (human playtests) needed to train a model.
Active learning makes the trade-off in playtesting between ``exploring'' potentially valuable game design settings and ``exploiting'' known good solutions with small changes explicit.
%Active learning complements the above approaches by providing a principled method for reducing the number of playtests needed without changing the fundamental models involved.
Thus, AL complements online game adaptation through reducing the number of mediocre or bad sets of game parameters players experience before arriving at good parameter settings without changing the underlying models used.




\subsection{Active Learning in Games}

There are other uses of AL in games.
% previous AL approaches
Normoyle et al. \cite{normoyle2012:al-metrics} use AL to recommend sets of useful player metrics to track; we pick design settings to improve player experience.
Rafferty et al. \cite{rafferty2012:opt-cog-game} optimize game designs offline to learn the most about player cognition; we use an online process and explore a variety of alternative AL approaches.
Our approach complements prior uses of AL for game design by focusing on efficiently improving designs for player behavior and experience.
We extend these efforts with a wider variety of AL methods while addressing the cost of playtesting.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Playtesting as Active Learning}

Our goal is to reduce the playtesting burden by efficiently choosing game designs to test on players.
Active learning (AL) provides a generic set of techniques to guide the playtesting process of choosing a set of design parameters to test toward achieving a design goal.
Playtesting typically involves trade-offs between testing designs that are poorly understood (exploration) and refining designs that are known to be good but need minor changes (exploitation).
Active learning captures this intuition through explicit models of the exploration-exploitation trade-off.
In this section we characterize playtesting in terms of AL.
We provide intuition behind AL, but full mathematical treatments are available through the references.


Machine-driven playtesting---active learning for game design---uses a model of how a design works to choose how to change that design to achieve a design goal.
Formally, the design goal is known as the \textit{objective function} and captures the relationship between input (game design parameters) and desired output (game design goals: specific player behaviors or subjective responses).
An \textit{acquisition function} takes information on expected outputs from the objective function and uses it to decide on the next set of inputs to test.

Objective functions come in two forms: regression and classification models.
\textit{Regression} models capture continuous outputs---e.g. how the rate of enemy firing in a shoot-`em-up influences the number of times a player is hit or how the height of jumps in a platformer influences how long it takes players to complete a level.
\textit{Classification} models capture discrete outputs---e.g. which of a pair of control settings a player preferred in a racing game or which choice a player made when given a set of options in a choose-your-own adventure.
Note that many design goals can be formulated as goals for playtesting: the primary challenge lies in defining a useful metric for measuring these goals through player feedback or in-game behavior.

Acquisition functions differ for regression and classification functions.
In the next sections we provide intuitive definitions of several acquisition functions---references to the relevant mathematical literature are provided.
Our survey of regression models covers key methods that balance between exploration of possible designs and exploitation to refine high quality designs.
For classification models we cover the most common frameworks for AL with discrete data that are intended to mitigate the impact of highly variable data.



\subsection{Regression Models}
%% regression
Acquisition functions choose which input parameters to test next.
Acquisition functions vary along a spectrum of exploration---playtesting by picking designs that are least understood---and exploitation---picking designs that are expected to be best.
We consider four acquisition functions for regression models: (1) variance, (2) probability of improvement, (3) expected improvement, and (4) upper-confidence bounds.
These acquisition functions have been developed in the field of Bayesian experimental design and apply generally to any regression model with a probabilistic interpretation \cite{chaloner1995}.
Regression models are useful when design goals fall along a continuous scale; we examine player behavior, specifically studying performance as damage taken.


Regression acquisition functions include:
\begin{description}
\item[Variance] Picks the input with greatest output uncertainty \cite{brochu2010:thesis}. 
Corresponds to picking the design that is hardest to predict the outcomes from. 
Exploration.
%
\item[Probability of improvement (PI)] Picks the input that most likely to improves the over the best previous output \cite{brochu2010:thesis}. 
Corresponds to picking the design most likely to improve over the current best. 
Exploitation.
%
\item[Expected Improvement (EI)] Picks the input by weighting the probability of output improvement by the amount of improvement \cite{brochu2010:thesis}.
Corresponds to picking the design with largest expected improvement.
Balances exploration and exploitation, but more computationally costly.
%
\item[Upper Confidence Bound (UCB)] Picks the input with combined greatest expected value and uncertainty \cite{srinivas2010:gp-ucb}.
Corresponds to picking designs that seem high quality but are poorly understood to gradually narrow the space of design to be known good or expected bad but uncertain.
The approach cited gradually reduces the weighting of uncertainty to help converge on an answer.
Balances exploration and exploitation.
\end{description}

% variance
%The \textit{variance} acquisition function picks designs based purely on the uncertainty of the design goal value \cite{brochu2010:thesis}.
%Intuitively, variance encodes a playtesting strategy of attempting any design that is poorly understood until the entire design space is well-understood.
%Variance approaches are pure exploration and are useful when design information on many possibilities is needed.

% probability of improvement
%\textit{Probability of improvement} (PI) improves on uncertainty by selecting the design that is most likely to gain some improvement of the best design observed so far \cite{brochu2010:thesis}. 
%Intuitively, probability of improvement is a playtesting strategy of attempting the design that seems most likely to improve over what has been tried so far.
%Probability of improvement approaches are pure exploitation and are effective when performing minor tweaks to already-effective designs.

% expected improvement
%\textit{Expected improvement} (EI) enhances PI by weighting the probability of each design improvement by the amount of improvement \cite{brochu2010:thesis}.
%%Unlike probability of improvement or variance, EI balances between exploration and exploitation.
%Intuitively, EI seeks designs that are likely to have large improvements over the existing design.
%%Mathematically EI is typically very computationally expensive, though for certain objective functions it has analytic solutions that are readily used.
%%Below we use one such objective function, the Gaussian Process \cite{rasmussen2006}.
%%By balancing exploration and exploitation EI is a more general-purpose technique useful for playtesting during many stages of the development cycle.
%Compared to PI, EI is more computationally expensive, though certain objective functions have cheap analytic solutions.

% upper confidence bounds
%\textit{Upper Confidence Bound} (UCB) methods select designs based on the upper bound on design quality according to an objective function model of design quality and variability \cite{srinivas2010:gp-ucb}.
%Designs that are expected to be high quality but are also poorly understood (and thus highly variable) are given priority.
%By testing these designs UCB narrows down the space of designs to those known to be good or unknown but likely to be bad.
%In the UCB approach cited above the importance of variability is gradually scaled down as more playtests are run.
%Intuitively, this capture the notion of initially exploring a variety of possible designs and gradually converging to exploit the most promising designs.

% some kind of wrap-up?


\subsection{Classification Models}
%% classification
Classification models are useful when design goals involve discrete choices; we examine player subjective ratings, specifically studying preference choices when picking between sets of controls.
We consider five acquisition functions for classification models: (1) entropy, (2) query-by-bagging voting, (3) query-by-bagging probability, (4) expected error reduction, and (5) variance reduction.
These acquisition functions have been developed for general classification models with only entropy requiring probabilistic predictions.
% entropy
% note: technically a subset of uncertainty sampling
% (1) least confident
% (2) margin-based
% (3) entropy
% (4) expected error reduction
% (5) variance reduction

\begin{description}
\item[Entropy] Picks input with greatest output uncertainty according to entropy---a measure of information needed to encode a distribution \cite{settles2012:al-book}.
Corresponds to picking designs where player choices are most uncertain.
Exploration.
%
\item[Query-By-Bagging (QBB)] Trains multiple copies of a classification model on random subsets of existing input-output data and picks next input with greatest disagreement among those models \cite{settles2012:al-book}.
Corresponds to picking designs with greatest variability in expected outcomes when viewed based on different subsets of playtest data.
Exploration.
Two varieties are relevant:
\begin{description}
\item[QBB vote] Each model votes on predicted output for each potential input.
The input with the greatest difference between its top two output options is then picked.
\item[QBB probability] Each model predicts the probability of each predicted output for each potential input.
The input with most uncertain averaged output probability is then picked \cite{abe1998:qbb}.
\end{description}
%
\item[Expected Error Reduction] Picks the input that, if used to train the model, will give greatest expected reduction in model error when predicting on the remaining inputs.
Corresponds to picking designs that will most improve prediction of the design outcomes over the design as a whole.
Balances exploration and exploitation but very computationally expensive.
The algorithm performs the following steps:
\begin{enumerate}
\item Pick a potential input and assign one of the possible outputs to make a ``fake'' training point.
\item Train a new classification model using the fake point and compute the error over the remaining input points.
\item Repeat (1) and (2) for all possible outputs and compute the average (``expected'') error across possible outputs.
\item Repeat (1) through (3) for all inputs.
\item Pick the input with greatest difference between expected error and current model expected error.
\end{enumerate}
%An unlabeled data point is selected and assigned one of the possible labels.
%This ``fake'' data point is used to train a new objective function model and the error of that model is calculated over all remaining unlabeled data points.
%Error is combined across possible labels for each unlabeled data point.
%Expected error reduction then picks the unlabeled point with the least average (``expected'') error.
\item[Expected Variance Reduction] Same as expected error reduction, but seeks to reduce variability in outputs rather than error.
Corresponds to picking designs that lead to greatest reduction in uncertainty about the design space over time.
\end{description}

%\textit{Entropy} acquisition functions choose designs that are most uncertain according to entropy \cite{settles2012:al-book}.
%Entropy measures the amount of information needed to encode a distribution.
%Intuitively, this captures how uncertain a model is about the possible (discrete) outcomes, emphasizing exploration of possibilities.
%Entropy sampling acts similarly to variance-based sampling for regression.


% QBB
% note: technically a subset of query-by-committee
% (1) QBC
% (2) QBB
%\textit{Query-By-Bagging} (QBB) approaches emphasize disagreement among potential models \cite{settles2012:al-book}.
%First, multiple copies of the same objective function are trained using random selections of the playtest data gathered so far.
%This captures the notion of guessing how the model might be if slightly different sets of playtests had occurred.
%Second, these models predict the outcome of new playtests.
%The final choice of design to test is the one these models most disagree on.
%Intuitively, this captures the idea of picking designs that are most likely to be poorly understood.

% QBB vote
% QBB probability
%Two varieties of QBB are relevant: QBB vote and QBB probability.
%In \textit{QBB vote} each objective function model votes on outcomes, giving a single choice.
%When only two options are possible (e.g. new controls were better or worse than previous) the design with the greatest disagreement between two binary outcomes is chosen.
%When multiple options are possible (e.g. a multi-way branch in a choose-your-own-adventure), the design with the greatest difference between the top two options is chosen.
%%
%In \textit{QBB probability} each objective function model estimates how probable each outcome is, giving a weight to all choices.
%The weights of all models are averaged and the point with lowest probability is chosen. \todo[inline]{find citation}

% expected error reduction
% variance reduction
%Expected error reduction and variance reduction attempt to maximize the quality of objective function predictions, rather than reduce variance or disagreement \cite{settles2012:al-book}.
%In \textit{expected error reduction} an unlabeled data point is selected and assigned one of the possible labels.
%This ``fake'' data point is used to train a new objective function model and the error of that model is calculated over all remaining unlabeled data points.
%Error is combined across possible labels for each unlabeled data point.
%Expected error reduction then picks the unlabeled point with the least average (``expected'') error.
%Intuitively, expected error reduction guesses how a new playtest could result and picks the playtest that will most improve the model of how people play.
%%
%\textit{Variance reduction} performs a similar process but minimizes the variance of predictions over data points, rather than the error.
%Intuitively, this picks a playtest most likely to increase certainty about how future playtests will go.

% TODO: KL-divergence or vote entropy?

% TODO: decision boundary distance for KSVM?

% TODO: more of uncertainty sampling?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Game Domain}

\begin{figure}[t]
\centering
%\includegraphics[width=1\linewidth]{./bullethell_sidebyside}
%\includegraphics[width=1\linewidth]{./bullethell_topbybottom}
\includegraphics[width=1\linewidth]{./bullethell_topbybottom_annotated}
\caption{Study game interface illustrating player, enemies, and shots fired by both at two points along adaptation process.}
%Preference learning experiments varied parameters related to player movement; enemy tuning involved the speed, size, and rate of fire of enemy bullets.
\label{fig:shmup}
\end{figure}

We sought to assess the efficacy of AL at reducing the number of playtests needed to achieve a design goal.
To conduct a case study of machine-driven playtesting we developed a simple shoot-`em-up game (Figure \ref{fig:shmup}).
Shoot-`em-up games emphasize reflexes and pattern recognition abilities as a player maneuvers a ship to dodge enemy shots and return fire.
%
In general, arcade games serve as an ideal starting domain for low-level parameter tuning:
\begin{itemize}
\item There are a number of parameters that can potentially interfere with each other: size and speed of enemies and enemy bullets, rate of enemy fire, player speed, player rate of fire, etc.
\item The game can be played in a series of waves, enabling our system to naturally test game parameter settings and gather player feedback.
\item Action-oriented gameplay reduces the complexity of player long-term planning and strategizing.
\item A scoring system makes gameplay goals and progress clear, unlike domains involving puzzle-solving or aesthetic enjoyment of a game world or setting.
\end{itemize}
%
In the case of shoot-`em-up games, we tested two different kinds of game design goals: (a) player game play behavior goals and (b) player subjective response goals.
Player game play behavior goals cover cases where designers seek particular play patterns or outcomes---e.g. player success rates or score achieved. 
Subjective responses goals cover cases where designers desire specific player subjective feedback---e.g. getting good user ratings on the feel of the controls.


% % enemy params
The shoot-`em-up game involves space ship combat over a series of waves.
During each wave a series of enemies appear that fire bullets at the player. 
To test AL for regression we set a game play behavior design goal of the player being hit exactly six times during each wave of enemies and tuned enemy parameters.
We varied the size of enemy bullets, speed of enemy bullets, and rate that enemies fire bullets. 
Increasing bullet size requires the player to move more carefully to avoid bullets. 
Faster bullets require quicker player reflexes to dodge incoming fire. 
More rapid firing rates increase the volume of incoming fire. 
Together these three parameters govern how much players must move to dodge enemy attacks, in turn challenging player reflexes. 
Getting approximate settings for these parameters is easy, but fine-tuning them for a desired level of difficulty can be challenging. 


% % control params
To test AL for classification we set a subjective response design goal of the player evaluating a set of controls as better than the previous set and tuned player control parameters.
We varied two ship movement parameters: drag and thrust. 
Drag is the ``friction'' applied to a ship that decelerates the moving ship at a constant rate when it is moving---larger values cause the ship to stop drifting in motion sooner. 
Thrust is the ``force'' a player movement press applies to accelerate the ship---larger values cause the ship to move more rapidly when the player presses a key to move. 
%
Combinations of thrust and drag are easy to tune to rough ranges of playability.
However, the precise values needed to ensure the player has the appropriate controls are difficult to find as player movement depends on how enemies attack and individual player preferences for control sensitivity (much like mouse movement sensitivity). 
After wave of enemies a menu asked players to indicate if the most recent controls were better, worse, or as good/bad as (``neither'') the previous set of controls.
We provided a fourth option of ``no different'' for when players could not distinguish the sets of controls, as opposed to ``neither'' where players felt controls differed but had no impact on their preferences.


%Our experiments investigate how well an AI system can learn the right parameter settings to achieve a desired level of player performance (in terms of rate of being hit by enemies) or a most preferred set of controls (in terms of player subjective responses).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
Our experiments tested whether AL could reduce the number of human playtests needed to tune design parameters compared to a random sampling approach.
Random sampling is the standard method used to evaluate the efficacy of AL models for improving model fits for a fixed number of playtests \cite{settles2012:al-book}. 
This is similar to A/B testing approaches that capture large amounts of data before acting on the results.

In two experiments we used the AL acquisition functions given above for regression by tuning enemy parameters and for classification by tuning player controls, respectively.
For both experiments we first built a dataset by providing human players with random sets of parameters and recording behavior or subjective responses.
The experiments had AL methods use this data as a potential pool of playtests to run and evaluated how well those methods could pick a sequence of playtests that gave the best performance.
%Both experiments involved a preliminary test on simulated data followed by testing with human participants.
%The simulation experiments allowed us to verify that our methods would work in principle; humans studies show our method can apply to real-world contexts.

In the regression study we used Gaussian Processes (GPs), the standard non-linear regression function used in the Bayesian experimental design literature.
Gaussian Processes generally yield good models with few playtests (samples) and have computationally inexpensive analytic formulations for many of our acquisition functions.
In the classification study we used three different objective functions---Gaussian Processes (GP), kernel support vector machines (KSVM), and optimized neural networks (``neuro-evolution'', NE).
KSVMs and NE are common classification approaches, whereas GPs are not.
%We added KSVMs and NE as GPs are not a common classification approach.
Kernel methods (particularly KSVMs and GPs) are a popular machine learning technique previously used in player modeling \cite{yu2011:minboredom} and optimized neural networks have been widely used in preference learning \cite{yannakakis2011:edpcg}.\footnote{For computational reasons we employ a simple gradient-based optimization method, rather than the more common neuro-evolutionary approaches. 
We did not find any performance differences between the two optimization approaches in initial tests on our data.}
%All three of these objective functions can be trained in regression or classification modes.
Note that NE does not produce probabilistic predictions, limiting the acquisition functions it can be paired with.


% enemy tuning summary
Our results show AL can reduce the number of human playtests needed.
For enemy parameter tuning (a regression problem) we find methods that balance exploration and exploitation (UCB and EI) have the best performance.
Variance performs well with many (280) playtests but not with few; likely due to the sensitivity of variance estimates on training data size.
%
% control tuning results summary
For control tuning (a classification problem) we find methods that tolerate high variance (e.g. QBB and entropy) have strong performance.
Of the objective functions GPs showed strong gains with few (100) playtests while KSVMs and NE showed greater gains with many (200) playtests.
%On human data GPs show good performance using QBB probability with few (roughly 80) samples while KSVMs show good performance using QBB probability with more (roughly 280) samples; NE showed the best performance overall and little difference among techniques used.
%Our control tuning results show a generic set of good controls for all players is difficult to obtain.
Together these results show AL has promise for helping to automate low-level parameter tuning.
In the following sections we detail our experimental design and then discuss the results of our tests in detail.


% % GP classification
% % entropy for sim; QBB prob for real (prob same)
% sim: best early classifier (0.78 @ 80 samples), same as KSVM w/more (0.85 @ 280)
% real: drop in performance to tied worst late classifier (0.78 @ 280); best early (0.74 @ 80)
%
% % GP regression
% % GP UCB for sim; GP UCB for real; variance gradually catches up, EI gets worse w/more (to point of random)
% sim: PI gets worse; min MSE/MAE do badly; EI/UCB/variance competitive, EI asymptotes better but more erratic w/fewer
%	UCB: 0.00128 @ 80; 0.0011 @ 280
%	EI: 0.00137 @ 80; 0.00098 @ 280
% real: GP UCB best (200 @ 80; 225 @ 280) 
%	note: random beats neuro random, ksvm random badly (260 @ 80 vs 510 vs 633; 240 @ 280 vs 500 vs 685)

% % KSVM classification
% % QBB vote for sim; QBB prob for real (prob same)
% sim: worst classifier (0.83 @ 280)
% real: best late classifier (0.81 @ 280)
%
% % KSVM regression
% sim: random best (0.0015 @ 80; 0.00097 @ 280); MSE/MAE both bad
% real: min MSE best (634 @ 80; 688 @ 280); gradually drops to random (min MAE always worse)

% % NE classification
% % QBB vote for sim; QBB vote for real (prob much worse, but doesn't make sense)
% best classifier 0.96 @ 200
% real: worst classifier (0.71 @ 80; 0.76 @ 280)
%
% % NE regression
% sim: random best (0.0016 @ 80; 0.0010 @ 280)
% real: random best (509 @ 80; 500 @ 280)


\subsection{Data Collection}

To perform an empirical evaluation of our methods we deployed two versions of our game online.
We publicized the game through websites and local emailing lists and did not offer compensation to participants.
In order to collect data on patterns of play over time we asked participants to try to play at least 10 waves of the game, though we did not enforce this requirement.

For analysis we only used data from players who reached at least 10 waves of play total.
This ensures we avoid data from players who were unable to reliably run the game.
% AZ 2013/10/29 - planning to rerun w/requirement of min 10 waves total, only waves 1-10
%	AZ 2013/11/27 - done
For our regression experiment this resulted in data from 138 players and 991 waves of the game total (using all waves each player played).
For our preference experiment we had 57 players, 47 of these provided only binary responses of ``better'' or ``worse'' and we limited our analysis to this subset of players yielding 416 paired comparisons.
% AZ 2013/10/29 - planning to rerun w/requirement of min 10 waves total; should filter some of the people with little data and thus more likely to be spurious results
%	AZ 2013/11/27 - done
We only used preference responses during the first 10 waves of play to avoid collecting many positive responses from those who were highly engaged in the game.
Note that we did not collect preference comparisons for the first wave of the game as players could not yet compare control settings.



\subsection{Experiment Design}

% experiment design
Using this data we performed 10-fold cross-validated experiments to measure how accurately an objective function could predict player responses (output) given a set of design parameters (input).
% regression
For regression we trained the GP using the three enemy parameters (bullet speed, bullet size, and firing rate) to predict the squared difference between the number of times the player was hit and a desired rate of 6 times.
We squared the difference to more steeply penalize sets of parameters with greater differences from the ideal.
% classification
For classification we trained the three objective functions with four parameters (current and previous drag and thrust) to predict whether the preference rating was ``better'' or ``worse.''
We discarded ratings that were not in these two classes as our data had too few samples to make a comparison (only 10/57 players ever responded in other categories).


For each cross-validation fold we first set aside a randomly selected 10\% of the data for evaluating objective function performance.
%randomly selected 300 data points to use for cross-validation; the remaining data was used as .
Next we randomly sampled 30 design parameter settings and responses from the other 90\% of the dataset to create a training data set; the remaining dataset samples formed the training pool.
%An additional set of 500 testing points was generated in the same way.
%We then created the grid of points the AL model could sample, forming the training pool.
Within each fold we then repeated the following process:
\begin{enumerate}
\item Train the objective function on the training data set.
\item Test the objective function on the testing data set and measure the error.
\item Use the acquisition function to pick a point from the training pool to improve the objective function model.
\item Move the selected point from the training pool into the training data.
\item Return to the first step and repeat the process until the maximum number of training points is used.
\end{enumerate}
\noindent We used a maximum of 300 training points in both regression and classification.\footnote{For computational reasons we only used a maximum of 200 points for error reduction and variance reduction and 280 NE methods in classification.}
For regression we measured error as the mean squared error from the desired rate of being hit.
For classification we measured error as the F1 score---a combination of model accuracy and coverage of sample data.






% regression: 138 players (currently using max 10 waves verion)
%% note: currently kept up to 10 waves, but allowed fewer
%% if min 10 waves -> 82 players, 1753 data points
%% if max 10 waves -> 138 players, 991 data points [current]
%% if both -> 82 players; 819 data points
% preference: 57 players (in random) with any value; 47 with binary choices
%% if binary but no wave -> 47 players, 318 points [current]
%% if binary min 10 waves -> 41 players, 296 points



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

Overall our results show AL is a promising approach for reducing the number of playtests needed to train an accurate predictive model.
In both simulation studies AL almost always reduced the number of playtests needed.
In both human studies AL was able to reduce the number of playtests in some, but not all, cases.
No single acquisition function, objective function, or acquisition-objective function pair was optimal across cases.
These results align with previous work in AL showing that many data-specific properties impact the results \cite{schein2007:al-logreg-eval}.
Below we provide further details with an emphasis on how AL impacted the need for playtesting.


\subsection{Regression}

\todo[inline]{fix}
Our regression experiments both show strong results for AL.
Having a clear behavioral objective (being hit a number of times in the game) was likely a strong contributor.
In both simulation and human data we found UCB and EI to be most effective (Table \ref{tab:reg_res}).
Both methods explicitly balance exploring and exploiting designs, suggesting many parameter tuning objectives involve a balance between considering alternative parameter settings and refining a given setting.

% http://www.tablesgenerator.com/

\begin{table}[tb]
\caption{Regression experiment: GP mean squared error at 80 or 280 samples with various acquisition functions. Lower values indicate better performance.}
\centering
\begin{tabular}{|c|c|c|}
\hline
acquisition function & 80 samples   & 280 samples  \\ \hline
random                                                        & 136          & 124          \\ \hline
variance                                                      & 121          & \textbf{120} \\ \hline
PI                                                            & 133          & 123          \\ \hline
EI                                                            & \textbf{112} & 127          \\ \hline
UCB                                                           & \textbf{107} & \textbf{117} \\ \hline
\end{tabular}
%\begin{tabular}{|c|c|l|c|l|}
%\hline
%\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}acquisition\\ function\end{tabular}}} & \multicolumn{2}{|c|}{simulation}                  & \multicolumn{2}{|c|}{human data}                 \\ \cline{2-5} 
%\multicolumn{1}{|l|}{}                                                                   & \multicolumn{1}{|l|}{\textbf{80}} & \textbf{280}  & \multicolumn{1}{|l|}{\textbf{80}} & \textbf{280} \\ \hline
%random                                                                                   & 1.91                              & 1.36          & 136                               & 124          \\ \hline
%variance                                                                                 & 1.50                              & 1.39          & 121                               & 120          \\ \hline
%PI                                                                                       & 8.32                              & 6.38          & 133                               & 123          \\ \hline
%EI                                                                                       & 1.60                              & \textbf{1.30} & 112                               & 127          \\ \hline
%UCB                                                                                      & \textbf{1.48}                     & 1.38          & \textbf{107}                      & \textbf{117} \\ \hline
%\end{tabular}
\label{tab:reg_res}
\end{table}


\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{regression_results}
\caption{Regression experiments GP mean squared error on predicting number of times player is hit vs number of samples used in training for different acquisition functions. Lower values indicate better performance. Top figure uses simulation data, bottom uses human data. Lines demarcate points used in Table \ref{tab:reg_res}.}
\label{fig:reg_all}
\end{figure}

\todo[inline]{get figure w/relative improvements}

%% human
% UCB and EI are only ones to do well
% -- UCB remains at consistently high performance
% -- EI gradually gets worse over time
% why?
% -- variance and PI do not capture exploration and thus have difficulty finding good results
% -- UCB is able to temper exploration and exploitation over time, likely helping focus on best areas to test
Methods that balance exploration and exploitation---UCB and EI---performed well, other AL methods did not (Figure \ref{fig:reg_all}).
Variance's lower performance is explained by the tendency to focus on highly uncertain parameter settings---in a high-dimensional space it is easy to find many sets of uncertain bad parameters, leading to poorer performance.
Over time EI's performance gradually decayed while GP-UCB maintained better performance.
As more samples are gathered GP-UCB reduces exploration while EI eventually begins to make poor playtest choices.
Approximately 60 samples were needed to train the successful AL methods to their peak performance; random sampling never achieved this level of performance on our data set.
Overall this clearly demonstrates AL can enhance playtesting efficacy, perhaps beyond what would happen through simply A/B testing and collecting data.

% summary
\todo[inline]{fix}
Our regression experiments demonstrate the power of AL to reduce the amount of playtesting required.
Methods that balance exploration and exploitation---EI and UCB---show the greatest efficacy across simulation and human studies.
These results make a strong case for AL applied to optimizing low-level in-game behaviors, such as difficulty in terms of in-game performance.


\subsection{Classification}
Our classification experiments show AL reduce testing needs by reducing variance in predictions.
%In simulation data variance-reducing methods---entropy and QBB vote---yielded improvements over random sampling.
%On human data similar methods were effective, often yielding larger performance improvements than in simulation.
\todo[inline]{update}
These results suggest AL methods are effective even with more complex data and can improve a variety of objective functions.


\begin{figure}[tbph]
\centering
\includegraphics[width=\linewidth]{classification_results}
\todo{replace}
\caption{Classification F1 score vs number of samples used in training for different objective and acquisition function combinations. Higher values indicate better performance. Top figure uses simulation data, bottom uses human data.}
\label{fig:cls_all}
\end{figure}


\begin{table}[tb]
\centering
\caption{Comparison of acquisition-objective function combination accuracy with few (100) and many (200) playtest samples.}
% % scriptsize version
\scriptsize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}acquisition\\ function\end{tabular}} & \multicolumn{3}{|c|}{100 samples}                & \multicolumn{3}{|c|}{200 samples}                \\ \cline{2-7} 
\multicolumn{1}{|l|}{}                                                          & GP             & KSVM           & NE             & GP             & KSVM           & NE             \\ \hline
random                                                                          & 0.720          & 0.684          & 0.673          & 0.773          & 0.709          & 0.718          \\ \hline
entropy                                                                         & \textbf{0.763} & \textbf{0.731} & N/A            & 0.763          & 0.751          & N/A            \\ \hline
QBB vote                                                                        & \textbf{0.758} & \textbf{0.746} & \textbf{0.703} & 0.780          & \textbf{0.777} & \textbf{0.760} \\ \hline
QBB prob                                                                        & 0.749          & 0.724          & N/A            & \textbf{0.792} & \textbf{0.782} & N/A            \\ \hline
error red                                                                       & \textbf{0.761} & 0.702          & N/A            & \textbf{0.795} & \textbf{0.772} & N/A            \\ \hline
var red                                                                         & 0.660          & 0.667          & N/A            & 0.725          & 0.723          & N/A            \\ \hline
\end{tabular}

% table of improvements
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline
%\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}acquisition\\ function\end{tabular}} & \multicolumn{3}{|c|}{100 samples}                & \multicolumn{3}{|c|}{200 samples}                \\ \cline{2-7} 
%\multicolumn{1}{|l|}{}                                                          & GP             & KSVM           & NE             & GP             & KSVM           & NE             \\ \hline
%entropy                                                                         & \textbf{0.043} & \textbf{0.046} & N/A            & -0.010         & 0.042          & N/A            \\ \hline
%QBB vote                                                                        & \textbf{0.038} & \textbf{0.062} & \textbf{0.030} & 0.007          & 0.068          & \textbf{0.042} \\ \hline
%QBB prob                                                                        & 0.029          & 0.040          & N/A            & \textbf{0.019} & \textbf{0.074} & N/A            \\ \hline
%error red                                                                       & \textbf{0.041} & 0.018          & N/A            & \textbf{0.023} & \textbf{0.065} & N/A            \\ \hline
%var red                                                                         & -0.060         & -0.017         & N/A            & -0.046         & 0.016          & N/A            \\ \hline
%\end{tabular}

%
% % non-scriptsize version
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline
%\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}acquisition\\ function\end{tabular}} & \multicolumn{3}{|c|}{80 samples} & \multicolumn{3}{|c|}{280 samples} \\ \cline{2-7} 
%\multicolumn{1}{|l|}{}                                                          & GP        & KSVM      & NE       & GP        & KSVM      & NE        \\ \hline
%random                                                                          & 0.71      & 0.65      & 0.68     & 0.76      & 0.75      & 0.74      \\ \hline
%entropy                                                                         & 0.71      & 0.69      & 0.68     & 0.76      & 0.75      & 0.77      \\ \hline
%QBB vote                                                                        & 0.71      & 0.70      & 0.70     & 0.78      & 0.79      & 0.78      \\ \hline
%QBB prob                                                                        & 0.71      & 0.70      & 0.68     & 0.78      & 0.79      & 0.74      \\ \hline
%\end{tabular}
\label{tab:cls_expr}
\end{table}

% human:
%	random: GP > NE > KSVM
%	AL: GP random/QBB prob > NE QBB vote > KSVM QBB prob
%		GP: QBB prob +vote wins w/few samples, loses/ties at more
%		NE: no difference
%		KSVM: QBB vote early improvement
In human studies entropy, QBB vote and probability, and error reduction all showed improvements over random sampling (Table \ref{tab:cls_expr}).
QBB methods (especially vote) were effective at both few and many samples.
Entropy was only effective with few samples while error reduction was most effective with more samples.
Error reduction depends on predicting future outcomes and thus requires more initial data before becoming effective.
When using random sampling GPs performed best, followed by NE and then KSVMs.
Using AL methods provided the largest performance boost for KSVMs, though GPs and NE also benefited.


% summary
Our classification experiments thus demonstrate AL can reduce the amount of playtesting needed even for subjective features of a design such as control settings.
Reducing playtest costs requires acquisition functions (e.g. QBB or entropy) that mitigate the high variance inherent in preference response data.
Exploring methods that learn multiple possible distinct control settings is a promising direction for future work.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}

% % note: can combine multiple aspects into single acquisition function, but then need additional complexity of learning how to balance objectives if not told directly; e.g. preference + performance

% % enable continuous model improvement

% % future: optimal experimental design (rafferty, chaloner); hcomp for more direct human participation

We have shown how low-level design parameter tuning playtesting can be automated.
Using AL techniques can reduce the expense of playtesting in terms of the number of input points (designs) tested.
Machine-driven playtesting can thus complement the strengths of human game designers by allowing them to focus on high-level design goals.
Machine-driven playtesting has great promise for broadening how automation enhances existing game design practices across the design iteration process.
In the future we hope to see automated techniques for other aspects of design like recognizing design flaws or assigning credit to aspects of a design for a variety of player behavioral and subjective outcomes.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Acknowledgments}
% Eric Fruchter - or possible co-author


\bibliographystyle{abbrv}
\bibliography{../latex/lib}  % keeping lib in separate github project

\end{document}
