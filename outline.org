* thesis
** low-level tuning AI to help design
   Design tools that automate time-consuming parts of process. 
   Help formalize playtesting process and notions of design goals.
   Reduce expense of playtesting, particularly in cases where testing is done remotely and with ``lower-resolution'' data (not specifics of everything done).
** formalize design goals + show process to automatically achieve
   ultimately helps designers move to collaborating with AI systems to balance roles and contributions
   opens questions in how to represent _process_ of designing for AI to collaborate - player metrics as one example
* related work
** design support
   hard constraints: nelson, smith, butler, bauer, shaker, g smith
   -- con: requires design to be integrated fully with tool to analyze
   -- con: typically limited to high levels of abstraction
   us
   -- focus on small-scale changes and tuning
   -- core to many processes and requires humans in the loop for many design goals (e.g. control ``feel'')
** game languages / tools
   for AI: vgdl, Twine, smith (VF), dormans, treanor
   -- good starting point for formalizing design
   -- con: not yet exploring how to effectively _support_ design
   us
   -- aiming to automate minor changes in design for designer use if the goals can be specified in specific metrics
   -- [future]: extend to allow looser specification of design goals, not single metrics
   -- [future]: leverage players via richer tuning feedback
** DDA [cut]
   rule-based: hunicke, magerko, thue, el-nasr
   -- con: need to manually specify
   us
   -- generalize automatically and tune directly
   -- could take rules and fine-tune how they work to best achieve design goals
** EC + ML [cut?]
   EC: stanley/hastings, shaker, yannakakis
   ML: missura, yu
   -- pro: automatically fit to player or player cluster
   -- con: little design control
   -- con: overfitting risk
   us
   -- go from optimizing to also exploring to find best
   -- can take same systems but build in process of seeking best alternatives to try (for individuals, groups, or whole population)
* active design optimization
  Working to reduce the playtesting burden and provide general approach to design optimization.
  Draw from active learning---techniques that try to guide the process of which design change to make next to optimize the design.
  Models trade-off between trying new designs that might work (exploration) and fine-tuning designs that are known to work (exploitation).
  Aim in this paper is to convey the intuitions behind techniques; detailed mathematical treatments are cited.
** active learning
   acquisition functions - how to evaluate what parameters to try next
   need different ones for regression + classification
   -- regression: minimize ``error'', here from ideal performance number
   -- classification: minimize misclassification, here prediction of player response
*** regression
    techniques to use when trying to get a specific quantity
    examples: how often a player wins; how many times an event (dependent on player action) happens in the game; proportion of players who learn a game concept
**** variance
     picking based on uncertainty
     trying out designs that are poorly understood (hard to guess what will happen)
**** PI [X]
     only picking based on probability
     trying only designs expected to do well (after minor tweak)
**** EI
     expectation over all possible outcomes to weight more important
     weighting outcomes by how good they are, not just how likely
**** UCB
     bandit optimization process with running model of certainty around parameters attempted
     starting out by exploring very different designs, then honing in on best designs. take into account that some designs might be okay, but could be great
*** classification
**** entropy
     pick by notion of how well-spread out
     want to pick designs highly unlike others
**** QBB
     go by disagreement among models (either final choice or probability of outcomes)
     take different perspectives on design by only using random parts of previous known design outcomes; if you had a room of copies of yourself who only knew randomly different parts of the past, what would they most agree/disagree on as a good decision?
** gaussian processes
   [could cut for the most part, neither a contribution nor essential for understanding it]
** ksvm
   [ditto - main point is KSVM doesn't have regression model that captures variance]
** ?neuroevol?
* game
** why arcade?
   many parameters that influence one another
   wave-based play to test and vary
   actions -> less planning / strategizing
   scoring -> goals are explicit
* experiments
  (1) random collection of data
  -- (a) simulation
  -- (b) humans
  (2) train model on collected data
  two domains
  (1) regression - vary bullet speed, size, firing rate to get player hit 6 times per wave
  (2) preference - vary control thrust + drag to get player to rate as ``better'' over time
** simulation methods
*** simulation description
    probabilistic model of skill response to parameters
    player: set of skills and single level of skill error
    -- each skill randomly selected from normal distribution, spread by skill error
    comparison: 
    -- skill-response is normal distribution of skill difference from parameter + some error
    -- overall response is average of those responses across skills, along with base failure rate
    --> models player as having 3 separate skills to handle different enemy parameters, but with fixed amount of ``slip-up''
    data pool:
    -- grid of evenly spaced points in parameter space for each bullet parameter
*** simulation description
    cf pref model paper
    player model: preferences for two parameters and shared error on preference (range of variability to tolerate)
    comparison
    -- underlying preference value is cumulative normal distribution of difference of parameter from desired spread by error term
    -- response for a given parameter is squared error number, compared between two parameter settings
    -- parameters combined by checking choices for both
    -- four outcomes
    (1) small difference (below error level) means ``no difference''
    --> if both are nodiff, then nodiff
    --> if one is nodiff, then give response from other
    (2) positive difference for newer gives ``better''
    --> both better or one better + one nodiff
    (3) negative difference gives ``worse''
    --> both worse or one worse + one nodiff
    (4) one better + one worse --> ``neither''
    captures same range of responses as in-game settings used for experiment
** human study methods
* results
** regression
*** simulation results
    all 3 methods do quite well
    EI asmpytotes to slightly better results
*** human study results
    UCB and EI are only ones to do well
    -- UCB remains at consistently high performance
    -- EI gradually gets worse over time
    why?
    -- variance and PI do not capture exploration and thus have difficulty finding good results
    -- UCB is able to temper exploration and exploitation over time, likely helping focus on best areas to test
** classification
*** simulation results
    GP random beats KSVM random -> GP is better able to model domain
    GP entropy shows best trend -> focus on finding out where problems are
    QBB models do moderately, but expensive to train -> best for high variance situations and model was not too high variance [cf Schein logreg]
    NE most effective; esp QBB vote
    NE random outdoes others
*** human study results
    all methods roughly equal
    GP random does well
    GP random beats KSVM random -> GP is better able to model domain
    no strong winners, domain was very hard
    KSVM entropy is slightly better 
    QBB models don't perform 
    --> QBB + KSVM results suggest consistent domain but complex underlying model
    only minor gains: ~160 samples to improve, little growth over random
* discussion
** AI for low-level design
** applications
   automating small aspects of design
   opening new ways of using design tools to seek out goals
   employing in automated game generation to guide process
